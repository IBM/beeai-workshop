{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced BeeAI for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Summarize a text document using text summarization</li> <li>Generate specific information from a large document using the RAG technique</li> <li>Predict future trends using time series forecasting</li> <li>Generate programming code (Bash) by prompting a code model</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Lab 0: Pre-work Pre-work for the workshop Lab 1: Document Summarization with Granite Learn how to use an AI model to summarize a work of literature Lab 2: Retrieval Augmented Generation (RAG) with Langchain Learn how to generate specific information from a large document Lab 3: Energy Demand Forecasting with Granite Timeseries (TTM) Learn how to predict future trends using time series forecasting Lab 4: Generating Bash Code with Granite Code Learn how to use an AI model to generate programming code"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>Google Colab</li> <li>IBM Granite AI foundation models</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> <li>Replicate</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>BJ Hargrave</li> <li>Martin Hickey</li> <li>Ming Zhao</li> <li>The notebooks used in this workshop are versions of notebooks from the IBM Granite Community modified for the workshop needs</li> </ul>"},{"location":"lab-1/","title":"Document Summarization with Granite","text":"<p>Text summarization condenses one or more texts into shorter summaries for enhanced information extraction.</p> <p>The goal of this lab is to show how you can use IBM Granite models in order to apply long document summarization techniques to a work of literature.</p>"},{"location":"lab-1/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-1/#lab","title":"Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter notebook notebooks/Summarize.ipynb\n</code></pre> <p>The path of the notebook file above is relative to the <code>granite-workshop</code> folder from the git clone in the pre-work.</p>"},{"location":"lab-1/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Document Summarization notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"lab-2/","title":"Retrieval Augmented Generation (RAG) with Langchain","text":"<p>Retrieval Augumented Generation (RAG) is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query.</p> <p>The goal of this lab is to show how you can use RAG with an IBM Granite model to augment the model query answer using a publicly available document.</p>"},{"location":"lab-2/#pre-requisite","title":"Pre-requisite","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-2/#lab","title":"Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter notebook notebooks/RAG_with_Langchain.ipynb\n</code></pre> <p>The path of the notebook file above is relative to the <code>granite-workshop</code> folder from the git clone in the pre-work.</p>"},{"location":"lab-2/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Retrieval Augmented Generation (RAG) with Langchain notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"lab-3/","title":"Energy Demand Forecasting with Granite Timeseries (TTM)","text":"<p>Forecasting in time series analysis allows data scientists to identify patterns by using machine learning and then generate forecasts about the future. TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting.</p> <p>The goal of this lab is to show how you can predict future trends on historical data using the IBM Granite Time Series models.</p>"},{"location":"lab-3/#pre-requisite","title":"Pre-requisite","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-3/#lab","title":"Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter notebook notebooks/Time_Series_Getting_Started.ipynb\n</code></pre> <p>The path of the notebook file above is relative to the <code>granite-workshop</code> folder from the git clone in the pre-work.</p>"},{"location":"lab-3/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Energy Demand Forecasting with Granite Timeseries (TTM) notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"lab-4/","title":"Generating Bash Code with Granite Code","text":"<p>Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on. A Large Language Model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation.</p> <p>The goal of this lab is to show how you can use prompt engineering with the IBM Granite Code LLMs in order to generate Bash programming language or code.</p>"},{"location":"lab-4/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-4/#lab","title":"Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter notebook notebooks/Text_to_Shell.ipynb\n</code></pre> <p>The path of the notebook file above is relative to the <code>granite-workshop</code> folder from the git clone in the pre-work.</p>"},{"location":"lab-4/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Generating Bash Code with Granite Code notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"pre-work/","title":"Pre-work","text":"<p>The labs in the workshop are Jupyter notebooks. The notebooks can be run on your computer or remotely on the Google Colab service. Check out Running the Granite Notebooks section on how to setup the way you want to run the notebooks.</p>"},{"location":"pre-work/#running-the-granite-notebooks","title":"Running the Granite Notebooks","text":"<p>The notebooks can be run:</p> <ul> <li>Locally on your computer OR</li> <li>Remotely on the Google Colab service</li> </ul> <p>Follow the instructions in one of the sections that follow on how you would like to run the notebooks.</p>"},{"location":"pre-work/#running-the-granite-notebooks-locally","title":"Running the Granite Notebooks Locally","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>If not, then it recommended to go to the Running the Granite Notebooks Remotely (Colab) section instead.</p> <p>Running the lab notebooks locally on your computer requires the following steps:</p> <ul> <li>Local Prerequisites</li> <li>Clone the Granite Workshop Repository</li> <li>Serving the Granite AI Models</li> <li>Install Jupyter</li> </ul>"},{"location":"pre-work/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Python 3.10, 3.11, or 3.12</li> </ul>"},{"location":"pre-work/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repo and cd into the repo directory.</p> <pre><code>git clone https://github.com/IBM/granite-workshop.git\ncd granite-workshop\n</code></pre>"},{"location":"pre-work/#serving-the-granite-ai-models","title":"Serving the Granite AI Models","text":"<p>Lab 1: Document Summarization with Granite, Lab 2: Retrieval Augmented Generation (RAG) with Langchain and Lab 4: Generating Bash Code with Granite Code require Granite models to be served by an AI model runtime so that the models can be invoked or called. There are 2 options to serve the models as follows:</p> <ul> <li>Replicate AI Cloud Platform OR</li> <li>Running Ollama Locally</li> </ul>"},{"location":"pre-work/#replicate-ai-cloud-platform","title":"Replicate AI Cloud Platform","text":"<p>Replicate is a cloud platform that will host and serve AI models for you.</p> <ol> <li> <p>Create a Replicate account. You will need a GitHub account to do this.</p> </li> <li> <p>Add credit to your Replicate Account (optional). To remove a barrier to entry to try the Granite models on the Replicate platform, use this link to add a small amount of credit to your Replicate account.</p> </li> <li> <p>Create a Replicate API Token.</p> </li> <li> <p>Set your Replicate API Token as an environment variable in your terminal where you will run the notebook:</p> <pre><code>export REPLICATE_API_TOKEN=&lt;your_replicate_api_token&gt;\n</code></pre> </li> </ol>"},{"location":"pre-work/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama. You will need to have a computer with:</p> <ul> <li>GPU processor</li> <li>At least 32GB RAM</li> </ul> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>If you computer is unable to serve the models, then it is recommended to go to the Replicate AI Cloud Platform section instead.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already. Ollama v0.3.14+ is required, so please upgrade if on an earlier version.</p> <p>On macOS, you can use Homebrew to install with</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite3.3:2b\nollama pull granite3.3:8b\n</code></pre> <p>For Lab 4: Generating Bash Code with Granite Code, you will also need at least one of the following Granite Code models.</p> <pre><code>ollama pull granite-code:3b\nollama pull granite-code:8b\n</code></pre> </li> </ol>"},{"location":"pre-work/#install-jupyter","title":"Install Jupyter","text":"<p>Use a virtual environment</p> <p>Before installing dependencies and to avoid conflicts in your environment, it is advisable to use a virtual environment (venv).</p> <ol> <li> <p>Create virtual environment:</p> <pre><code>python3 -m venv --upgrade-deps --clear venv\n</code></pre> </li> <li> <p>Activate the virtual environment by running:</p> <pre><code>source venv/bin/activate\n</code></pre> </li> <li> <p>Install Jupyter notebook in the virtual environment:</p> <pre><code>python3 -m pip install --require-virtualenv notebook ipywidgets\n</code></pre> <p>For more information, see the Jupyter installation instructions</p> </li> <li> <p>To open a notebook in Jupyter (in the active virtual environment), run:</p> <pre><code>jupyter notebook &lt;notebook-file-path&gt;\n</code></pre> </li> </ol>"},{"location":"pre-work/#running-the-granite-notebooks-remotely-colab","title":"Running the Granite Notebooks Remotely (Colab)","text":"<p>Running the lab notebooks remotely using Google Colab requires the following steps:</p> <ul> <li>Colab Prerequisites</li> <li>Serving the Granite AI Models for Colab</li> </ul> <p>!!! note \"Notebook execution speed tip\" The default execution runtime in Colab uses a CPU. Consider using a different Colab runtime to increase execution speed, especially in situations where you may have other constraints such as a slow network connection. From the navigation bar, select <code>Runtime-&gt;Change runtime type</code>, then select either GPU- or TPU-based hardware acceleration.</p>"},{"location":"pre-work/#colab-prerequisites","title":"Colab Prerequisites","text":"<ul> <li>Google Colab requires a Google account that you're logged into</li> </ul>"},{"location":"pre-work/#serving-the-granite-ai-models-for-colab","title":"Serving the Granite AI Models for Colab","text":"<p>Lab 1: Document Summarization with Granite, Lab 2: Retrieval Augmented Generation (RAG) with Langchain and Lab 4: Generating Bash Code with Granite Code require Granite models to be served by an AI model runtime so that the models can be invoked or called.</p>"},{"location":"pre-work/#replicate-ai-cloud-platform-for-colab","title":"Replicate AI Cloud Platform for Colab","text":"<p>Replicate is a cloud platform that will host and serve AI models for you.</p> <ol> <li> <p>Create a Replicate account. You will need a GitHub account to do this.</p> </li> <li> <p>Add credit to your Replicate Account (optional). To remove a barrier to entry to try the Granite Code models on the Replicate platform, use this link to add a small amount of credit to your Replicate account.</p> </li> <li> <p>Create a Replicate API Token.</p> </li> <li> <p>Add your Replicate API Token to the Colab Secrets manager to securely store it. Open Google Colab and click on the \ud83d\udd11 Secrets tab in the left panel. Click \"New Secret\" and enter <code>REPLICATE_API_TOKEN</code> as the key, and paste your token into the value field. Toggle the button on the left to allow notebook access to the secret.</p> </li> </ol>"}]}