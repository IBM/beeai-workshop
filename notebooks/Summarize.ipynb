{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization\n",
    "\n",
    "This notebook demonstrates an application of long document summarization techniques to a work of literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "### Install Dependencies\n",
    "\n",
    "Granite Kitchen comes with a bundle of dependencies that are required for notebooks. See the list of packages in its [`setup.py`](https://github.com/ibm-granite-community/granite-kitchen/blob/main/setup.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/granite-kitchen \\\n",
    "    transformers \\\n",
    "    torch \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the Granite AI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook requires IBM Granite models to be served by an AI model runtime so that the models can be invoked or called. This notebook can use a locally accessible [Ollama](https://github.com/ollama/ollama) server to serve the models, or the [Replicate](https://replicate.com) cloud service.\n",
    "\n",
    "During the pre-work, you may have either started a local Ollama server on your computer, or setup Replicate access and obtained an [API token](https://replicate.com/account/api-tokens). In this case, you can skip the \"Running Ollama in Colab\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Ollama in Colab\n",
    "\n",
    "This section is if you are not going to either run the Ollama server locally on your computer or use the Replicate cloud service. Running the Ollama server in Colab will limit the size of Granite models you can use and be _significantly_ slower when calling the Granite models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Download and install Ollama in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start the Ollama server as a background process in Colab using `nohup` and `&`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"nohup ollama serve &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pull down the Granite models in Colab that you will use in the workshop. Larger models take more memory to run. The `20b` model is too large for the Colab runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull granite-code:3b\n",
    "!ollama pull granite-code:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Select a Granite model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.\n",
    "\n",
    "When using Replicate, if the `REPLICATE_API_TOKEN` environment variable is not set, or a `REPLICATE_API_TOKEN` Colab secret is not set, then the notebook will ask for your [Replicate API token](https://replicate.com/account/api-tokens) in a dialog box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import set_env_var\n",
    "\n",
    "try: # Look for a locally accessible Ollama server for the model\n",
    "    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n",
    "    model = OllamaLLM(model=\"granite-code:8b\")\n",
    "except Exception: # Use Replicate for the model\n",
    "    set_env_var(\"REPLICATE_API_TOKEN\")\n",
    "    model = Replicate(model=\"ibm-granite/granite-8b-code-instruct-128k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Download a book\n",
    "\n",
    "Here we fetch H.D. Thoreau's \"Walden\" from [Project Gutenberg](https://www.gutenberg.org/) for summarization.\n",
    "\n",
    "We have to trim it down so that it will fit in the context window of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JFi40LArpIa"
   },
   "outputs": [],
   "source": [
    "# The following URL contains a text version of H.D. Thoreau's \"Walden\"\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205.txt\"\n",
    "\n",
    "# Get the contents\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "full_contents = response.text\n",
    "\n",
    "# Extract the text of the book, leaving out the gutenberg boilerplate.\n",
    "start_str = \"*** START OF THE PROJECT GUTENBERG EBOOK WALDEN, AND ON THE DUTY OF CIVIL DISOBEDIENCE ***\"\n",
    "start_index = full_contents.index(start_str) + len(start_str)\n",
    "end_str = \"*** END OF THE PROJECT GUTENBERG EBOOK WALDEN, AND ON THE DUTY OF CIVIL DISOBEDIENCE ***\"\n",
    "end_index = full_contents.index(end_str)\n",
    "book_contents = full_contents[start_index:end_index]\n",
    "print(f\"Length of book text: {len(book_contents)} chars\")\n",
    "\n",
    "# We limit the text to 10k characters, which is about 2.8k tokens.\n",
    "char_limit = 10000\n",
    "contents = book_contents[:char_limit]\n",
    "print(f\"Length of text for summarization: {len(contents)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "## Count the tokens\n",
    "\n",
    "Before sending our code to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the [`granite-8b-code-instruct-128k`](https://huggingface.co/ibm-granite/granite-8b-code-instruct-128k) model, which has a context window of 128,000 tokens.\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model.\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Your model uses the tokenizer \" + type(tokenizer).__name__)\n",
    "\n",
    "print(f\"Your document has {len(tokenizer.tokenize(contents))} tokens. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygNmITWQZAZ8"
   },
   "source": [
    "## Summarize the text\n",
    "\n",
    "We construct our final prompt and send it to the AI model served by Ollama for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu4HeuqWqvOj"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the following text:\n",
    "{contents}\n",
    "\"\"\"\n",
    "\n",
    "output = model.invoke(\n",
    "    prompt,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 10000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Summaries\n",
    "\n",
    "Here we use an iterative summarization technique to adapt to the context length of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text\n",
    "\n",
    "Divide the full text into smaller passages for separate processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text = book_contents\n",
    "print(f\"The text is {len(tokenizer.tokenize(text))} tokens.\")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunk_token_limit = 3000  # In tokens: 3000 message + 512 completion + ~350 padding < 4000 context length\n",
    "text_splitter = TokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=chunk_token_limit, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"Chunk count: {len(chunks)}\")\n",
    "print(f\"Max chunk tokens: {max([len(tokenizer.tokenize(chunk)) for chunk in chunks])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the chunks\n",
    "\n",
    "Here we create a separate summary of each passage. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "max_chunks = min(10, len(chunks)) # adjust to limit the work\n",
    "for i in range(max_chunks):\n",
    "    text = chunks[i]\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following text using only the information found in the text:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "    print(f\"{i + 1}. Prompt size: {len(tokenizer.tokenize(prompt))} tokens\")\n",
    "    output = model.invoke(\n",
    "        prompt,\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 2000, # Set the maximum number of tokens to generate as output.\n",
    "            \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "            \"temperature\": 0.75,\n",
    "            \"system_prompt\": \"You are a helpful assistant.\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0\n",
    "        }\n",
    "    )\n",
    "    print(f\"{i + 1}. Output size: {len(tokenizer.tokenize(output))} tokens\")\n",
    "    summary = f\"Summary {i+1}:\\n{output}\\n\\n\"\n",
    "    summaries.append(summary)\n",
    "    print(summary)\n",
    "\n",
    "print(f\"Summary count: {len(summaries)}\")\n",
    "summary_contents = \"\\n\\n\".join(summaries)\n",
    "print(f\"Total: {len(tokenizer.tokenize(summary_contents))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Summaries\n",
    "\n",
    "We signal to the model that it is receiving separate summaries of passages from an original text, and to create a unified summary of that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "A text was summarized in separate passages; those passage summaries are provided below.\n",
    "\n",
    "{summary_contents}\n",
    "\n",
    "From these summaries alone, compose a single, unified summary of the text.\n",
    "\"\"\"\n",
    "print(f\"Prompt size: {len(tokenizer.tokenize(prompt))} tokens\")\n",
    "print(prompt)\n",
    "output = model.invoke(\n",
    "    prompt,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 500, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
